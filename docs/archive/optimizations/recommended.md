# Recommended Performance Optimizations

**Date**: 2026-01-07 (Updated with AVX-512 results and removal)
**CPU**: AMD Ryzen 9 7950X (Zen 4, znver4)
**Architecture**: x86_64

**Status**: AVX-512 optimizations completed - VPOPCNTDQ kept, JSON parser removed

---

## CPU Feature Analysis

### Available SIMD Features ‚úÖ

Your Ryzen 9 7950X has exceptional SIMD capabilities:

```
‚úÖ SSE/SSE2/SSE3/SSSE3/SSE4.1/SSE4.2  - Universal x86_64 baseline
‚úÖ AVX/AVX2                            - 256-bit SIMD (currently used)
‚úÖ AVX-512F/BW/DQ/VL                   - 512-bit SIMD (NOT USED YET!)
‚úÖ AVX-512 VBMI/VBMI2                  - Vector byte manipulation
‚úÖ AVX-512 VNNI                        - Vector neural network
‚úÖ AVX-512 BITALG                      - Bit algorithms
‚úÖ AVX-512 VPOPCNTDQ                   - 8x parallel popcount ‚≠ê
‚úÖ BMI1/BMI2                           - Bit manipulation (FAST on Zen 4!)
‚úÖ POPCNT/LZCNT                        - Bit counting
‚úÖ VAES/VPCLMULQDQ                     - Vector crypto
```

**Key finding**: You have **AVX-512** with **VPOPCNTDQ** - this is rare and extremely valuable!

---

## Current Status

### ‚úÖ Good News: Runtime Dispatch Already Enabled!

Checking `src/json/simd/mod.rs`, I found:
```rust
#[cfg(all(target_arch = "x86_64", any(test, feature = "std")))]
pub fn build_semi_index_standard(json: &[u8]) -> SemiIndex {
    if is_x86_feature_detected!("avx2") {
        avx2::build_semi_index_standard(json)  // ‚úÖ Using AVX2!
    } else if is_x86_feature_detected!("sse4.2") {
        sse42::build_semi_index_standard(json)
    } else {
        x86::build_semi_index_standard(json)
    }
}
```

**Status**: Runtime dispatch is available via `std` feature (line 58-78)

### ‚ö†Ô∏è Issue: `std` Not in Default Features

From `Cargo.toml`:
```toml
[features]
default = []  # ‚Üê Empty! std not enabled by default
std = []      # ‚Üê Exists but must be explicitly enabled
```

**Impact**:
- Library users get SSE2 by default (slow)
- Must explicitly use `--features std` to get AVX2 dispatch
- CLI tool gets `std` (via `cli` feature), so it's fast

---

## Priority Recommendations

### ‚úÖ COMPLETED: Make `std` a Default Feature

**Status**: Already completed (Cargo.toml line 12: `default = ["std"]`)

**Impact**:
- ‚úÖ All library users get AVX2 runtime dispatch automatically
- ‚úÖ `no_std` users can opt-out: `default-features = false`

---

### ‚úÖ COMPLETED: AVX-512 Optimizations and Cleanup

**Status**: Implemented, benchmarked, and cleaned up (2026-01-07)

**Results**:
- ‚úÖ **AVX512-VPOPCNTDQ**: 5.2x speedup (104.4 GiB/s vs 20.3 GiB/s) - **KEPT**
- ‚ùå **AVX-512 JSON Parser**: 7-17% slower than AVX2 (672 vs 732 MiB/s) - **REMOVED**

**Actions Taken**:
- ‚úÖ Removed `src/json/simd/avx512.rs` (627 lines of slower code)
- ‚úÖ Removed `benches/json_avx512_comparison.rs`
- ‚úÖ Updated runtime dispatch to prioritize AVX2 > SSE4.2 > SSE2
- ‚úÖ All 508 tests pass

**Key Learning**: Wider SIMD ‚â† automatically faster. JSON parsing is memory-bound with sequential state machine dependencies. Failed optimizations should be removed, not kept as technical debt.

**Documentation**:
- [avx512-vpopcntdq-results.md](avx512-vpopcntdq-results.md) - 5.2x speedup details
- [avx512-json-results.md](avx512-json-results.md) - Why it was slower and removed
- [performance-outcomes-summary.md](performance-outcomes-summary.md)

---

### üü¢ NEW PRIORITY: BMI1/BMI2 Bit Manipulation (Based on AVX-512 Learnings)

**Why**: AVX-512 taught us that JSON parsing bottleneck is mask processing and branches, not SIMD width.

**Your CPU Status**:
- ‚úÖ BMI1: Fast on all CPUs
- ‚úÖ BMI2: **Fast on Zen 4** (3 cycles) - Intel-class performance
- ‚ö†Ô∏è BMI2 on Zen 1/2: Slow (18 cycles microcode) - must detect and avoid

**Target**: The actual bottleneck identified by AVX-512 benchmarks

#### üî¥ Priority 1: BMI1 JSON Mask Processing (2-3 hours)

**Problem Identified by AVX-512 Benchmarks**:
- JSON state machine processes masks bit-by-bit
- Current: Loop through all 32 positions, check each mask
- Bottleneck: Branch-heavy, checks many zero bits

**Solution - TZCNT + BLSR**:
```rust
// Current AVX2 approach (src/json/simd/avx2.rs)
fn process_chunk(class: CharClass, state: State, ...) {
    for i in 0..32 {
        let bit = 1u32 << i;
        let is_quote = (class.quotes & bit) != 0;  // Check every position
        let is_open = (class.opens & bit) != 0;
        // ... 5 more mask checks per byte
    }
}

// BMI1-optimized approach
#[target_feature(enable = "bmi1")]
unsafe fn process_chunk_bmi1(class: CharClass, state: State, ...) {
    use core::arch::x86_64::*;

    let mut combined_mask = class.quotes | class.opens | class.closes |
                            class.delims | class.value_chars;

    while combined_mask != 0 {
        let pos = _tzcnt_u32(combined_mask);        // Find next structural char
        combined_mask = _blsr_u32(combined_mask);   // Clear this bit

        // Determine character type and process (reduced branches)
        let is_quote = (class.quotes >> pos) & 1;
        let is_open = (class.opens >> pos) & 1;
        // ... process based on position
    }
}
```

**Expected Impact**: 5-10% JSON parsing speedup
- Fewer branches (only process structural characters)
- Better branch prediction
- Simpler control flow

**Effort**: 2-3 hours
**Risk**: Low (BMI1 fast on all CPUs)

---

#### üü° Priority 2: BMI2 Advanced Mask Processing (4-6 hours)

**Problem**: Select operations need k-th set bit, mask manipulation is complex

**Solution - PDEP/PEXT**:
```rust
// PEXT: Extract set bits to contiguous positions
// PDEP: Deposit bits to sparse positions

// Select optimization
#[target_feature(enable = "bmi2")]
unsafe fn select_bmi2(words: &[u64], k: usize) -> Option<usize> {
    // Use PDEP to create mask with k-th bit set
    // Combine with broadword tricks for faster search
}

// JSON mask parallelization
#[target_feature(enable = "bmi2")]
unsafe fn process_masks_parallel(masks: &CharClass) {
    // Use PDEP/PEXT to rearrange bits for parallel processing
    // Build lookup tables for fast state transitions
}
```

**Expected Impact**: 10-15% JSON parsing (on Zen 3+/Intel)

**Critical**: Must detect Zen 1/2 and avoid:
```rust
fn should_use_bmi2() -> bool {
    if !is_x86_feature_detected!("bmi2") {
        return false;
    }

    // Check AMD family/model via CPUID
    // Zen 1/2: Family 17h - avoid BMI2
    // Zen 3+: Family 19h+ - use BMI2
    if is_amd_zen1_or_zen2() {
        return false;  // 18-cycle microcode
    }

    true
}
```

**Effort**: 4-6 hours (including CPU detection)
**Risk**: Medium (complex, must handle Zen 1/2)

---

#### Comparison: Why BMI1/BMI2 vs AVX-512?

**AVX-512 Failed Because**:
- Targeted SIMD width (classification)
- But classification wasn't the bottleneck
- Mask processing and state machine dominated
- Wider SIMD added overhead

**BMI1/BMI2 Should Succeed Because**:
- Targets actual bottleneck (mask iteration, branches)
- Scalar optimizations for scalar work
- TZCNT/BLSR reduce branch mispredictions
- PDEP/PEXT enable better algorithms
- Works with bottleneck, not against it

**Expected Results**:
- BMI1: 5-10% speedup (likely)
- BMI1 + BMI2: 15-20% speedup (on Zen 3+/Intel)
- vs AVX-512: -3% (tried wrong approach)

---

### ‚úÖ COMPLETED: AVX512-VPOPCNTDQ for Popcount

**Status**: Implemented in `src/popcount.rs` (2026-01-07)

**Result**: 5.2x speedup (96.8 GiB/s vs 18.5 GiB/s)

**Real-world Impact**: Minimal (~1% overall) - popcount is only 1.6% of BitVec construction time

**Why Limited**: Amdahl's Law - optimizing 1.6% of work yields ~1% overall improvement

**Documentation**: See [avx512-vpopcntdq-results.md](avx512-vpopcntdq-results.md)

---

## Updated Implementation Roadmap (Post-AVX-512 Work)

### ‚úÖ Completed (2026-01-07)

1. **‚úÖ Make `std` default feature** - Done
   - Impact: 1.8x JSON parsing speedup via AVX2 runtime dispatch

2. **‚úÖ Implement AVX512-VPOPCNTDQ** - Done
   - Impact: 5.2x popcount throughput (minimal end-to-end due to Amdahl's Law)

3. **‚úÖ Implement AVX-512 JSON Parser** - Done (but not recommended for production)
   - Result: 3% slower than AVX2 (memory-bound, sequential dependencies)
   - Learning: Wider SIMD doesn't help memory-bound workloads

### üü¢ Next Priority: BMI1/BMI2 (Based on Learnings)

4. **BMI1 JSON Mask Processing** (2-3 hours)
   - Target: Actual bottleneck (mask iteration, branches)
   - Expected: 5-10% JSON parsing speedup
   - Risk: Low (BMI1 fast on all CPUs)

5. **BMI2 Advanced Optimizations** (4-6 hours)
   - Requires: CPU detection for Zen 1/2 avoidance
   - Expected: 10-15% JSON parsing (on Zen 3+/Intel)
   - Risk: Medium (complex, must handle slow BMI2 CPUs)

### üîµ Lower Priority

6. **Fix AVX-512 dispatch priority** (30 minutes)
   - Change runtime dispatch to prefer AVX2 over AVX-512 for JSON
   - Current: AVX-512 first (slower)
   - Recommended: AVX2 first (faster)

**Total potential improvement: 3-4x across the board**

---

## Testing Strategy

### Before Each Optimization

```bash
# Baseline benchmark
cargo bench --bench json_simd > baseline.txt
cargo bench --bench rank_select >> baseline.txt

# Ensure all tests pass
cargo test
cargo test --features simd
cargo test --test simd_level_tests
```

### After Each Optimization

```bash
# Benchmark with optimization
cargo bench --bench json_simd > optimized.txt
cargo bench --bench rank_select >> optimized.txt

# Compare
cargo install cargo-benchcmp
cargo benchcmp baseline.txt optimized.txt

# Verify correctness
cargo test --all-features
```

### Validate SIMD Level Selection

```bash
# Check which SIMD is used at runtime
RUST_LOG=debug cargo run --features cli -- json generate 1mb | head

# Force specific level for testing
cargo test --test simd_level_tests -- --nocapture
```

---

## Compilation Flags for Maximum Performance

### For Development/Testing
```bash
# Use default (allows all CPU types)
cargo build --release
```

### For Deployment (Single Known CPU)
```bash
# Enable ALL features for this exact CPU
RUSTFLAGS="-C target-cpu=native" cargo build --release

# This enables:
# - AVX-512
# - BMI2
# - All available optimizations
# No runtime dispatch needed (slightly faster, smaller binary)
```

### For Distribution (Multiple CPU Types)
```bash
# Use runtime dispatch (current approach)
cargo build --release --features std

# Binary auto-detects: AVX-512 > AVX2 > SSE4.2 > SSE2
# Slightly larger binary, but works everywhere
```

---

## Risk Assessment

### Very Low Risk (Do Now)
- ‚úÖ Make `std` default feature
- ‚úÖ AVX512-VPOPCNTDQ implementation

**Why**: Additive changes, well-understood, hardware support guaranteed

### Low Risk (Do Soon)
- ‚úÖ AVX-512 JSON parser

**Why**: Similar to existing AVX2, hardware support confirmed, easily testable

### Medium Risk (Optional)
- ‚ö†Ô∏è BMI2 integration

**Why**: Bit manipulation is tricky, but Zen 4 has fast BMI2

### High Risk (Research Phase)
- ‚ö†Ô∏è SIMD prefix sum state machine

**Why**: Complex algorithm, hard to debug, significant time investment

---

## Immediate Action Items (This Week)

### Priority 1: Enable `std` by Default (2 minutes)

```bash
# Edit Cargo.toml
sed -i 's/default = \[\]/default = ["std"]/' Cargo.toml

# Test
cargo test
cargo build --release

# Verify dispatch is active
cargo test --test simd_level_tests

# Commit
git add Cargo.toml
git commit -m "Enable std feature by default for runtime SIMD dispatch

This enables automatic CPU feature detection for JSON parsing,
providing 1.5-1.8x speedup on AVX2-capable CPUs (95% of systems).

Users needing no_std can disable: default-features = false"
```

### Priority 2: Benchmark Current Performance (15 minutes)

```bash
# Generate benchmark suite
cargo run --features cli -- json generate-suite --max-size 100mb --clean

# Run comprehensive benchmarks
cargo bench --bench json_simd > benchmarks/baseline-$(date +%Y%m%d).txt
cargo bench --bench rank_select >> benchmarks/baseline-$(date +%Y%m%d).txt

# Document current performance
git add benchmarks/
git commit -m "Add baseline performance benchmarks for Ryzen 9 7950X"
```

### Priority 3: Implement AVX512-VPOPCNTDQ (3-4 hours)

See implementation details above. This is the highest ROI for effort.

---

## Summary

**Your CPU is exceptional** - AMD Ryzen 9 7950X with full AVX-512 support including VPOPCNTDQ.

**Current status**: Runtime dispatch exists but `std` not default, no AVX-512 implementation

**Recommended immediate actions**:
1. ‚úÖ Make `std` default (2 min, 1.8x gain)
2. ‚úÖ Implement AVX512-VPOPCNTDQ (4 hours, 2-4x rank speedup)
3. ‚úÖ Implement AVX-512 JSON parser (8 hours, 3x total gain)

**Total potential**: **3-4x performance improvement** with 12-16 hours of work

**Next steps**: Start with Priority 1 (2 minutes), benchmark, then implement Priority 2 and 3.
