# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## AI Scratch Directory

When working on this project, use `.ai/scratch/` for temporary files:

- **Location**: `.ai/scratch/` in the repository root
- **Purpose**: Store intermediate work, drafts, analysis notes, generated data
- **Ignored by git**: The `.ai/` directory is in `.gitignore`

**Usage examples**:
- Draft documentation before finalizing
- Store benchmark results for comparison
- Keep notes about investigation/debugging sessions
- Save generated test data temporarily

Create the directory if it doesn't exist:
```bash
mkdir -p .ai/scratch
```

## Project Overview

Succinctly is a high-performance Rust library implementing succinct data structures with fast rank and select operations, optimized for both x86_64 (POPCNT) and ARM (NEON) architectures.

## Common Commands

### Building and Testing
```bash
# Standard build
cargo build

# Build with specific popcount strategy
cargo build --features simd
cargo build --features portable-popcount

# Run tests
cargo test

# Run tests with large bitvectors
cargo test --features large-tests
cargo test --features huge-tests
cargo test --features mmap-tests

# Run benchmarks
cargo bench

# Run property tests (longer-running)
cargo test --test property_tests
cargo test --test properties
```

### Testing Individual Components
```bash
# Test specific module
cargo test bitvec
cargo test bp
cargo test json

# Test a single test function
cargo test test_rank1_simple

# Test all SIMD levels explicitly
cargo test --test simd_level_tests

# Run benchmarks for specific operation
cargo bench rank_select
cargo bench json_simd
cargo bench balanced_parens
```

### CLI Tool

```bash
# Build CLI tool
cargo build --release --features cli

# Generate synthetic JSON for benchmarking
./target/release/succinctly json generate 10mb -o benchmark.json
./target/release/succinctly json generate 1mb --pattern pathological -o worst-case.json

# Query JSON files
./target/release/succinctly json query '.users[].name' -i input.json
./target/release/succinctly json query '.users[0]' -i input.json --raw
./target/release/succinctly json query '.items[]' -i input.json --mmap
```

## Code Architecture

### Core Data Structures

**BitVec** ([src/bitvec.rs](src/bitvec.rs))
- Main bitvector with rank/select support
- Memory layout: raw words (`Vec<u64>`), rank directory (~3% overhead), select index (~1-3% overhead)
- Uses 3-level Poppy-style rank directory (L0/L1/L2) for O(1) rank queries
- Select uses sampled index with configurable sample rate (default: 256)

**RankSelect trait** ([src/lib.rs](src/lib.rs))
- `rank1(i)`: Count 1-bits in `[0, i)` - O(1)
- `select1(k)`: Find position of k-th 1-bit - O(log n) with acceleration
- `rank0(i)`: Count 0-bits in `[0, i)` - O(1) (computed as `i - rank1(i)`)

### Balanced Parentheses

**BalancedParens** ([src/bp.rs](src/bp.rs))
- Succinct tree navigation using balanced parentheses encoding (1=open, 0=close)
- RangeMin structure with hierarchical min-excess indices (~6% overhead)
- Tree operations: `find_close`, `find_open`, `enclose`, `first_child`, `next_sibling`, `parent`, `excess`

### JSON Semi-Indexing

**JSON Module** ([src/json/mod.rs](src/json/mod.rs))
- Converts JSON text to Interest Bits (IB) and Balanced Parentheses (BP) vectors
- Two cursor implementations: `simple` (3-state) and `standard` (4-state)
- SIMD acceleration: AVX2 > SSE4.2 > SSE2 on x86_64, NEON on aarch64

**SIMD Module Structure** ([src/json/simd/](src/json/simd/))
- `x86.rs`: SSE2 baseline (16 bytes/iteration, universal on x86_64)
- `sse42.rs`: SSE4.2 with PCMPISTRI (16 bytes/iteration, ~90% availability)
- `avx2.rs`: AVX2 256-bit processing (32 bytes/iteration, ~95% availability) **← Fastest**
- `neon.rs`: ARM NEON (16 bytes/iteration, mandatory on aarch64)
- `mod.rs`: Runtime CPU feature detection and dispatch (AVX2 > SSE4.2 > SSE2)

### jq Query Module

**Current Implementation** ([src/jq/](src/jq/))
- `expr.rs`: AST for jq expressions
- `parser.rs`: Recursive descent parser
- `eval.rs`: Expression evaluator using cursor-based navigation

**Supported jq syntax**:
- `.` - Identity
- `.foo` - Field access
- `.[n]` - Array index (positive and negative)
- `.[n:m]`, `.[n:]`, `.[:m]` - Array slicing
- `.[]` - Iterate all elements
- `.foo?` - Optional access
- `.foo.bar`, `.foo[0].bar` - Chained expressions

## Feature Flags

**Popcount strategies** (mutually exclusive):
- Default: Rust's built-in `count_ones()`
- `simd`: Explicit SIMD intrinsics
- `portable-popcount`: Portable bitwise algorithm

**Other features**:
- `large-tests`: Test with 1GB bitvectors
- `huge-tests`: Test with 5GB bitvectors
- `mmap-tests`: Memory-mapped file tests

## Testing Strategy

**Unit tests**: In each module's `#[cfg(test)] mod tests`

**Property tests**:
- [tests/property_tests.rs](tests/property_tests.rs)
- [tests/properties.rs](tests/properties.rs)
- [tests/bp_properties.rs](tests/bp_properties.rs)

**Integration tests**:
- [tests/json_indexing_tests.rs](tests/json_indexing_tests.rs)
- [tests/bp_coverage_tests.rs](tests/bp_coverage_tests.rs)
- [tests/simd_level_tests.rs](tests/simd_level_tests.rs)

## `no_std` Support

The library is `no_std` compatible (except in tests):
- Uses `#![cfg_attr(not(test), no_std)]`
- Depends on `alloc` for `Vec<u64>` storage

## Benchmark Infrastructure

```bash
# Generate benchmark data
./target/release/succinctly json generate-suite
./target/release/succinctly json generate-suite --max-size 10mb

# Run benchmarks
cargo bench --bench json_simd
cargo bench --bench balanced_parens
```

### Benchmark Patterns
| Pattern | Description |
|---------|-------------|
| comprehensive | Mixed content (realistic) |
| users | User records (nested objects) |
| nested | Deep nesting (tests BP) |
| arrays | Large arrays (tests iteration) |
| strings | String-heavy (tests escapes) |
| unicode | Unicode strings |
| pathological | Worst-case |

## CI/CD

```bash
# Mirror CI checks locally
cargo clippy --all-targets --all-features -- -D warnings
cargo test
./scripts/build.sh
```

## AVX-512 Performance Learnings

### Key Insight: Wider SIMD ≠ Automatically Faster

Two AVX-512 optimizations implemented with dramatically different results:

#### ✅ AVX512-VPOPCNTDQ: 5.2x Speedup (Compute-Bound)

**Implementation**: [src/popcount.rs](src/popcount.rs)
- Processes 8 u64 words (512 bits) in parallel
- Hardware `_mm512_popcnt_epi64` instruction
- **Result**: 96.8 GiB/s vs 18.5 GiB/s (scalar) = **5.2x faster**

**Why it wins**: Pure compute-bound, embarrassingly parallel, no dependencies

**Real-world impact**: Minimal - popcount is only ~1.6% of BitVec construction time

#### ❌ AVX-512 JSON Parser: 7-17% Slower than AVX2 (Memory-Bound) - REMOVED

**Previous Implementation**: ~~`src/json/simd/avx512.rs`~~ (removed 2026-01-07)
- Processed 64 bytes/iteration (vs 32 for AVX2)
- **Result**: 672 MiB/s vs 732 MiB/s (AVX2) = **8.9% slower**
- **Action**: Implementation removed; runtime dispatch now prioritizes AVX2

**Why AVX2 won**:
1. **Memory-bound workload**: Waiting for data from memory, not compute
2. **AMD Zen 4 architecture**: Splits AVX-512 into two 256-bit micro-ops (not native 512-bit)
3. **State machine overhead**: Wider SIMD = more bytes to process sequentially afterward
4. **Cache alignment**: 32-byte chunks fit cache lines better than 64-byte
5. **Amdahl's Law**: Speeding up 20% (SIMD classification) doesn't help when 80% (state machine + memory) dominates

### Performance Comparison Table

| Workload | AVX-512 Result | Status | Reason |
|----------|---------------|--------|---------|
| Popcount | **5.2x faster** ✓ | **KEPT** | Compute-bound, parallel, no dependencies |
| JSON parsing | **7-17% slower** ✗ | **REMOVED** | Memory-bound, sequential state machine |
| Rank queries | **Minimal impact** | N/A | Popcount only 1.6% of total time |
| BitVec construction | **~1% faster** | N/A | Dominated by memory allocation + indexing |

### Architectural Insights

**AMD Zen 4 AVX-512 characteristics**:
- ✓ No frequency throttling (unlike early Intel AVX-512)
- ✗ Not native 512-bit (two 256-bit execution units)
- ✗ AVX-512 ops split into 2x 256-bit micro-ops
- → Additional latency from operation splitting
- → AVX2 uses full-width units directly

**When to use AVX-512**:
- ✓ Pure compute: math, crypto, compression
- ✓ No memory bottlenecks
- ✓ No sequential dependencies
- ✓ Data-parallel algorithms
- ✗ Memory-bound workloads
- ✗ Sequential state machines
- ✗ Complex control flow

### Benchmarking Best Practices

Based on this work:

1. **Always profile on target hardware** - Theoretical ≠ actual performance
2. **Test multiple SIMD widths** - Wider isn't always better
3. **Measure end-to-end impact** - Not just micro-optimizations
4. **Consider Amdahl's Law** - Optimize the bottleneck, not the fast path
5. **Understand workload characteristics** - Compute-bound vs memory-bound matters more than instruction width

### Documentation

Detailed analysis available in:
- [docs/AVX512-VPOPCNTDQ-RESULTS.md](docs/AVX512-VPOPCNTDQ-RESULTS.md) - Popcount 5.2x speedup
- [docs/AVX512-JSON-RESULTS.md](docs/AVX512-JSON-RESULTS.md) - Why AVX2 beats AVX-512 for JSON
- [docs/optimization-opportunities.md](docs/optimization-opportunities.md) - CPU feature analysis
- [docs/RECOMMENDED-OPTIMIZATIONS.md](docs/RECOMMENDED-OPTIMIZATIONS.md) - Prioritized optimization roadmap

### Production Recommendations

**For popcount operations**:
- Use AVX512-VPOPCNTDQ when available (Intel Ice Lake+, AMD Zen 4+)
- Runtime dispatch automatically selects best implementation
- Minimal real-world impact due to Amdahl's Law

**For JSON parsing**:
- **Use AVX2 as highest priority** (despite current runtime dispatch favoring AVX-512)
- AVX2 is 3-6% faster on Zen 4
- Broader compatibility (2013+ vs 2019+/2022+)
- Lower power consumption
- Keep AVX-512 as reference implementation and future optimization target

### Command Reference

```bash
# Test AVX-512 popcount implementation
cargo test --lib --features simd popcount

# Benchmark popcount strategies (includes AVX-512 VPOPCNTDQ)
cargo bench --bench popcount_strategies --features simd

# Generate JSON benchmark suite
cargo run --release --features cli -- json generate-suite

# Run comprehensive JSON benchmarks (AVX2, SSE4.2, SSE2, Scalar)
cargo bench --bench json_simd
```

### Key Takeaways for Future Optimizations

1. **Profile first, optimize second** - Don't assume wider/smarter is better
2. **Understand bottlenecks** - Memory-bound vs compute-bound matters
3. **Measure end-to-end** - Micro-benchmarks can be misleading
4. **Consider architecture** - Zen 4 splits AVX-512, future Zen 5 may not
5. **Amdahl's Law always wins** - Optimize what matters (the slow 80%), not what's easy (the fast 20%)
6. **Remove failed optimizations** - Slower code creates technical debt

### Failed Optimizations

See [docs/FAILED-OPTIMIZATIONS.md](docs/FAILED-OPTIMIZATIONS.md) for detailed analysis:
- **AVX-512 JSON parser**: -7-17% slower (removed 2026-01-07)
- **BMI1 mask iteration**: -25-31% slower (reverted 2026-01-07)
